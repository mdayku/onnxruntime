# ONNX Autodoc CI Pipeline
# Runs linting, type checking, and unit tests for the model inspection tool

name: Autodoc CI

on:
  push:
    paths:
      - 'tools/python/util/autodoc/**'
      - 'tools/python/model_inspect.py'
      - 'tools/python/util/model_inspect.py'
      - '.github/workflows/autodoc-ci.yml'
  pull_request:
    paths:
      - 'tools/python/util/autodoc/**'
      - 'tools/python/model_inspect.py'
      - 'tools/python/util/model_inspect.py'
      - '.github/workflows/autodoc-ci.yml'
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint:
    name: Lint & Type Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff mypy black
          pip install onnx numpy protobuf

      - name: Check formatting with Black
        run: |
          black --check --diff tools/python/util/autodoc/

      - name: Lint with Ruff
        run: |
          ruff check tools/python/util/autodoc/

      - name: Type check with mypy
        run: |
          mypy tools/python/util/autodoc/ \
            --ignore-missing-imports \
            --no-error-summary \
            --show-error-codes \
            || echo "::warning::Type checking found issues (non-blocking)"
        continue-on-error: true  # Type errors are warnings for now

  test:
    name: Unit Tests
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ['3.10', '3.11']

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov
          pip install onnx numpy protobuf

      - name: Run unit tests
        run: |
          cd tools/python/util
          pytest autodoc/tests/ -v --tb=short --color=yes
        env:
          PYTHONPATH: ${{ github.workspace }}/tools/python/util

      - name: Run tests with coverage
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.10'
        run: |
          cd tools/python/util
          pytest autodoc/tests/ --cov=autodoc --cov-report=term-missing --cov-report=xml

      - name: Upload coverage report
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.10'
        uses: codecov/codecov-action@v4
        with:
          files: tools/python/util/coverage.xml
          flags: autodoc
          fail_ci_if_error: false
        continue-on-error: true

  integration:
    name: Integration Test
    runs-on: ubuntu-latest
    needs: [lint, test]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install onnx numpy protobuf

      - name: Create test model
        run: |
          python -c "
          import numpy as np
          import onnx
          from onnx import TensorProto, helper

          # Create a simple Conv-BN-Relu model
          X = helper.make_tensor_value_info('X', TensorProto.FLOAT, [1, 3, 224, 224])
          W = helper.make_tensor('W', TensorProto.FLOAT, [64, 3, 7, 7],
                                 np.random.randn(64, 3, 7, 7).astype(np.float32).flatten().tolist())
          scale = helper.make_tensor('scale', TensorProto.FLOAT, [64], np.ones(64, dtype=np.float32).tolist())
          bias = helper.make_tensor('bias', TensorProto.FLOAT, [64], np.zeros(64, dtype=np.float32).tolist())
          mean = helper.make_tensor('mean', TensorProto.FLOAT, [64], np.zeros(64, dtype=np.float32).tolist())
          var = helper.make_tensor('var', TensorProto.FLOAT, [64], np.ones(64, dtype=np.float32).tolist())
          Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1, 64, 112, 112])

          conv = helper.make_node('Conv', ['X', 'W'], ['conv_out'], kernel_shape=[7, 7], strides=[2, 2], pads=[3, 3, 3, 3])
          bn = helper.make_node('BatchNormalization', ['conv_out', 'scale', 'bias', 'mean', 'var'], ['bn_out'])
          relu = helper.make_node('Relu', ['bn_out'], ['Y'])

          graph = helper.make_graph([conv, bn, relu], 'test_model', [X], [Y], [W, scale, bias, mean, var])
          model = helper.make_model(graph, opset_imports=[helper.make_opsetid('', 17)])
          onnx.save(model, 'test_model.onnx')
          print('Created test_model.onnx')
          "

      - name: Test CLI - Basic inspection
        run: |
          cd tools/python
          python model_inspect.py ../../test_model.onnx
        env:
          PYTHONPATH: ${{ github.workspace }}/tools/python/util

      - name: Test CLI - JSON output
        run: |
          cd tools/python
          python model_inspect.py ../../test_model.onnx --out-json report.json
          cat report.json | head -50
        env:
          PYTHONPATH: ${{ github.workspace }}/tools/python/util

      - name: Test CLI - Markdown output
        run: |
          cd tools/python
          python model_inspect.py ../../test_model.onnx --out-md model_card.md
          cat model_card.md
        env:
          PYTHONPATH: ${{ github.workspace }}/tools/python/util

      - name: Test CLI - Hardware estimates
        run: |
          cd tools/python
          python model_inspect.py ../../test_model.onnx --hardware a100 --precision fp16 --batch-size 8
        env:
          PYTHONPATH: ${{ github.workspace }}/tools/python/util

      - name: Test CLI - List hardware profiles
        run: |
          cd tools/python
          python model_inspect.py --list-hardware
        env:
          PYTHONPATH: ${{ github.workspace }}/tools/python/util

      - name: Verify outputs exist
        run: |
          test -f tools/python/report.json
          test -f tools/python/model_card.md
          echo "All integration tests passed!"
